# Deep-Fake API

This project is a FastAPI backend that runs deep-fake detection on videos.  
You send a short video â†’ it extracts frames â†’ detects faces â†’ runs them through a deep learning model (ResNeXt + BiLSTM) â†’ and returns if the video is FAKE or REAL with a confidence score.

âš ï¸ **Note:** The model weights (`checkpoint.pth`) are **not in this repo**.  
I keep them in the Drive:  
`IMAP > publication > amine > checkpoint` (you can chose the old one or the new one) 
You need to download the checkpoint before using the API. ( if you don't have the checkpoint, itâ€™s useless to launch the api it will not launch without having the model trained )  

---

## ðŸ”§ Tech Stack

- **FastAPI + Uvicorn** â†’ REST API
- **PyTorch** â†’ deep learning runtime
- **OpenCV / MEDIA PIPE /dlib** â†’ video decoding, frame extraction, face detection
- **Model** â†’ ResNeXt (for spatial features) + BiLSTM (for temporal sequence consistency)

PS : for the faces_cropped I have used Media pipe as a priority before using open CV
---

## ðŸ“‚ Repo structure

- `apiForAppNewOne.py` â†’ main FastAPI app  ( you can rename it at api if you want, I name it like that because it's to the new model) 
- `deepfake_model.py`, `model_definition.py` â†’ model loader + forward pass  
- `video_utils.py` â†’ video preprocessing, face detection, normalization  
- `usage_example.py` â†’ client example to call the API  
- `requirements.txt` â†’ dependencies  
- `benchmark_*` + `apiBenchmark.py` â†’ quick perf tests (latency/throughput)  
- `videos/` and `SHORT_VIDEO_HD/` â†’ demo input videos
- 'static' this folder is used to store split frames and cropped faces  

---

## ðŸš€ How to run

### 1) Setup
```bash
python -m venv .venv
source .venv/bin/activate        # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt  # probably you have some conflict about dependence but it's fine
pip install -r requirements.lock.txt # 
